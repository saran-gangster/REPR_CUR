seed_everything: 52

trainer:
  max_steps: 10000
  log_every_n_steps: 50
  check_val_every_n_epoch: 3
  gradient_clip_val: 1.0
  precision: bf16-mixed

model:
  vocab_size: 8192
  d_model: 512
  n_layers: 8
  n_heads: 8
  dropout: 0.1
  ff_multiplier: 4
  use_rope: true
  rope_base: 10000
  weight_tying: false

  jepa:
    loss_type: soft_nce_psr

    temperature: 0.15
    teacher_kernel_temp: 0.25
    neighbor_topk: 32
    softnce_uncert_scale: 1.0

    kappa_beta: 1.0
    kappa_temp: 1.5

    var_weight: 1.0
    cycle_weight: 0.0
    comp_weight: 0.0
    pres_weight: 0.05

    kl_future_weight: 0.1
    kl_temp: 2.0
    js_warmup_gamma: 2.0
    freeze_lm_head_for_kl: true

    latent_dim: 512
    horizons: [2, 8, 32, 64]
    horizon_probs: [0.4, 0.3, 0.2, 0.1]
    pairs_per_seq: 32
    ema_momentum: 0.996
    ema_schedule: [0.99, 0.9999]

    tap_layer: -4
    tap_norm: true
    grad_barrier: true

    jepa_weight: 0.05
    lm_weight: 0.8
    lm_weight_use_scheduler: false

  optimizer:
    lr: 1.5e-4
    weight_decay: 0.1
    betas: [0.9, 0.95]
    warmup_steps: 1500

data:
  module: wikitext2           
  data_dir: data/wikitext2/
  block_size: 512
  batch_size: 16
  num_workers: 4
  train_fraction: 1.0
  subset_seed: 1234
  bpe_vocab_size: 8192
  bpe_lowercase: true