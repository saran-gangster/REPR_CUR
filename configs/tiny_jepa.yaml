seed_everything: 52

trainer:
  max_steps: 5000
  log_every_n_steps: 25
  val_check_interval: 500
  check_val_every_n_epoch: null
  gradient_clip_val: 1.0
  precision: 32
  accelerator: auto
  devices: auto
  strategy: auto

model:
  vocab_size: 1024
  d_model: 256
  n_layers: 6
  n_heads: 8
  dropout: 0.1
  ff_multiplier: 4
  use_rope: true
  rope_base: 10000
  weight_tying: false

  jepa:
    latent_dim: 256
    predictor_hidden_multiplier: 1.5
    horizons: [2,8,32]
    horizon_probs: [0.6, 0.3, 0.1]
    pairs_per_seq: 64
    ema_momentum: 0.995
    gamma_var: 1.0
    gamma_cov: 1.0
    tau: 0.1
    tap_layer: -3
    grad_barrier: true
    tap_norm: true
    jepa_weight: 1.0
    lm_weight: 1.0
    # Set this to true for a clean LM-only baseline:
    # - JEPA disabled
    # - LM weight scheduler OFF (constant weight from step 0)
    run_baseline: false

    # Schedules (ignored when run_baseline=true)
    # IMPORTANT: no hard "off" phase; ramp from 0 from step 0
    lm_warmup_steps: 0
    # EMA momentum cosine schedule from start -> end over lm_warmup_steps (kept; applies to projector)
    ema_schedule: [0.99, 0.9995]

  optimizer:
    lr: 5.0e-5
    lr_head: 5.0e-6
    weight_decay: 0.1
    betas: [0.9, 0.95]
    warmup_steps: 200

data:
  data_dir: data/
  block_size: 512
  batch_size: 32
  num_workers: 2
  download_url: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
  # Standardized on a self-training HF BPE tokenizer
  bpe_vocab_size: 1024
  bpe_lowercase: true