##################################################
# Minimal LeJEPA config for Tiny Shakespeare
# 
# Core philosophy:
# - Simple prediction loss (cosine)
# - One geometric regularizer (VICReg)
# - Total loss = lm_loss + α * jepa_loss
#   where jepa_loss = (1-λ) * pred + λ * geom
##################################################

seed_everything: 52

trainer:
  max_steps: 5000
  log_every_n_steps: 25
  check_val_every_n_epoch: 3
  gradient_clip_val: 1.0
  precision: 32
  accelerator: auto
  devices: auto
  strategy: auto

model:
  vocab_size: 1024
  d_model: 256
  n_layers: 6
  n_heads: 8
  dropout: 0.1
  ff_multiplier: 4
  use_rope: true
  rope_base: 10000
  weight_tying: false

  jepa:
    # Core hyperparameters (only 2!)
    alpha: 0.1                    # JEPA weight: total_loss = lm + α*jepa
    geometry_lambda: 0.2          # Geometry weight: jepa = (1-λ)*pred + λ*geom
    
    # Architecture
    latent_dim: 256
    predictor_hidden_multiplier: 2.0
    
    # Horizons
    horizons: [2, 8, 32, 64]
    horizon_probs: [0.4, 0.3, 0.2, 0.1]
    pairs_per_seq: 64
    
    # Loss types
    prediction_loss_type: cosine  # "cosine" or "l2"
    geometry_regularizer: vicreg  # "vicreg" or "sigreg"
    
    # VICReg params (only used if geometry_regularizer=vicreg)
    vicreg_var_weight: 1.0
    vicreg_cov_weight: 0.1
    
    # SIGReg params (only used if geometry_regularizer=sigreg)
    sigreg_num_directions: 128
    
    # Tap settings
    tap_layer: -3
    tap_norm: true

  optimizer:
    lr: 1.0e-4
    weight_decay: 0.1
    betas: [0.9, 0.95]
    warmup_steps: 200

data:
  module: tiny_shakespeare
  data_dir: data/
  block_size: 512
  batch_size: 32
  num_workers: 2
  download_url: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
  bpe_vocab_size: 1024
  bpe_lowercase: true